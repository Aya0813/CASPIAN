{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "878dd95b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Force install into the active Jupyter environment (This guarantees it installs tensorflow-addons into the exact Python environment your notebook is using.)\n",
        "# import sys\n",
        "# !{sys.executable} -m pip install tensorflow-addons\n",
        "# !{sys.executable} -m pip install tensorflow\n",
        "# !{sys.executable} -m pip install numpy\n",
        "# !{sys.executable} -m pip install matplotlib\n",
        "# !{sys.executable} -m pip install scikit-learn\n",
        "\n",
        "# # successfully installed packages \n",
        "\n",
        "# !{sys.executable} -m pip install keras==2.1 # installing the version mentioned in the git repo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89e9e3a-296a-4eb8-92d9-a0c3e16bd343",
      "metadata": {
        "id": "f89e9e3a-296a-4eb8-92d9-a0c3e16bd343"
      },
      "outputs": [],
      "source": [
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import time\n",
        "from models import models as new_models\n",
        "from utils import *\n",
        "import keras\n",
        "\n",
        "from keras_unet_collection import models, utils\n",
        "import matplotlib.pyplot as plt\n",
        "print('TensorFlow {}; Keras {}'.format(tf.__version__, keras.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f16a1eb-2ea8-470d-a1da-ec6e37692261",
      "metadata": {
        "id": "7f16a1eb-2ea8-470d-a1da-ec6e37692261"
      },
      "source": [
        "## Constants, Parameters and Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5846bdd3-227d-4aa7-9e15-8b4285ac3ba0",
      "metadata": {
        "id": "5846bdd3-227d-4aa7-9e15-8b4285ac3ba0"
      },
      "outputs": [],
      "source": [
        "grid_size = 1024                # Set the size of the full input grid (likely 1024x1024 pixels)\n",
        "AUTOTUNE = tf.data.AUTOTUNE     # Set TensorFlow's automatic tuning for dataset loading (optimizes performance)\n",
        "batch_size = 2                  # Set the batch size (number of samples per training step)\n",
        "split = 1                       # Choose which data split to use (1, 2, or 3 as available)\n",
        "output_1d = False               # Whether the output labels should be flattened into 1D or kept 2D (here: 2D output)\n",
        "EPOCHS = 200                    # Number of training epochs (complete passes over the training dataset)\n",
        "\n",
        "\n",
        "# MODEL_NAME = \"Attn-Unet-pretrained\"\n",
        "# LR = 0.00015\n",
        "# MIN_LR = LR/5\n",
        "# WARMUP_EPOCHS = 20\n",
        "\n",
        "MODEL_NAME = \"SWIN-Unet\"        # Name of the model architecture to use\n",
        "LR = 0.00018                    # Set initial learning rate for the optimizer\n",
        "MIN_LR = LR/6                   # Set the minimum learning rate for learning rate decay                 \n",
        "WARMUP_EPOCHS = 20              # Number of epochs to spend \"warming up\" before reaching the initial LR\n",
        "\n",
        "# MODEL_NAME = \"CASPIAN\"\n",
        "# LR = 0.0008\n",
        "# MIN_LR = LR/10\n",
        "# WARMUP_EPOCHS = 20\n",
        "\n",
        "\n",
        "###### For Ablation studies ###########################\n",
        "\n",
        "MODEL_NAME = \"CASPIAN_beta_v4\"\n",
        "LR = 0.0008\n",
        "MIN_LR = LR/10\n",
        "WARMUP_EPOCHS = 20\n",
        "version = 4\n",
        "###### For Ablation studies ###########################\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Data Loading Stage\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "\n",
        "# If the output is not 1D, load and process a flood mask (used for post-processing model outputs)\n",
        "if not output_1d:\n",
        "    #Loading and processing the mask\n",
        "    the_mask = get_the_mask()\n",
        "\n",
        "\n",
        "# Load and preprocess the training and validation datasets\n",
        "ds = {\n",
        "    'train': tf.data.Dataset.load(\"./data/train_ds_aug_split_%d\" % split).map(lambda f,x,y,yf: tf.py_function(clear_ds,\n",
        "                                           inp=[f,x,y,yf, output_1d],\n",
        "                                           Tout=[tf.float32, tf.float32])),\n",
        "    'val': tf.data.Dataset.load(\"./data/val_ds_aug_split_%d\" % split).map(lambda f,x,y,yf: tf.py_function(clear_ds,\n",
        "                                           inp=[f,x,y,yf, output_1d],\n",
        "                                           Tout=[tf.float32, tf.float32]))\n",
        "}\n",
        "# at the end -> both ds['train'] and ds['val'] are the training and validation datasets cleaned and ready to use\n",
        "\n",
        "\n",
        "# If the model is an Attention U-Net, adjust the input channels accordingly\n",
        "if \"Attn\" in MODEL_NAME:\n",
        "    ds['train'] = ds['train'].map(lambda x,y: expand_input_channels(x, y))\n",
        "    ds['val'] = ds['val'].map(lambda x,y: expand_input_channels(x, y))\n",
        "\n",
        "# Print the number of elements in the training set\n",
        "print(\"Size of the Training Dataset: %d  \" % tf.data.experimental.cardinality(ds[\"train\"]).numpy())\n",
        "\n",
        "# Print the number of elements in the validation set\n",
        "print(\"Size of the Validation Dataset: %d  \" % tf.data.experimental.cardinality(ds[\"val\"]).numpy())\n",
        "\n",
        "# Take one sample from the training set and print its input shape and unique input values\n",
        "input_sample, label_sample = next(iter(ds[\"train\"]))\n",
        "print(\"Training input shape: %s\" % str(input_sample.numpy().shape))\n",
        "print(\"Training input values: %s\" % str(np.unique(input_sample.numpy())))\n",
        "\n",
        "# Print the corresponding output (label) shape\n",
        "print(\"Training output shape: %s \" % str(label_sample.numpy().shape))\n",
        "\n",
        "# Repeat the same for the validation set\n",
        "input_sample, label_sample = next(iter(ds[\"val\"]))\n",
        "print(\"Validation input shape: %s\" % str(input_sample.numpy().shape))\n",
        "print(\"Validation output shape: %s \" % str(label_sample.numpy().shape))\n",
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# Plot samples from the training set\n",
        "# -----------------------------------------\n",
        "\n",
        "# Number of samples to visualize\n",
        "samples = 2\n",
        "\n",
        "# Create a figure with 2 rows and 2 columns\n",
        "fig, axs = plt.subplots(samples,2, facecolor='w', edgecolor='k')\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "#fig.subplots_adjust(hspace = .2, wspace=.13)\n",
        "fig.tight_layout(pad=2, h_pad=2.5, w_pad=2.5)\n",
        "\n",
        "# Flatten the axes array for easier iteration\n",
        "axs = axs.ravel()\n",
        "\n",
        "# Initialize index for subplots\n",
        "i = 0\n",
        "\n",
        "# Loop through two samples from the training dataset\n",
        "for element in list(ds['train'].as_numpy_iterator())[:samples]:\n",
        "    x,y = element\n",
        "    axs[i].imshow(x, origin=\"lower\", interpolation='none', aspect='auto')\n",
        "    axs[i].set_title(\"Sample input\")\n",
        "    i += 1\n",
        "    axs[i].imshow(y, origin=\"lower\", interpolation='none', aspect='auto')\n",
        "    axs[i].set_title(\"Sample output\")\n",
        "    i += 1\n",
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# Prepare datasets for training\n",
        "# -----------------------------------------\n",
        "\n",
        "# Batch the datasets (combine samples into batches of size batch_size)\n",
        "ds['train'] = ds['train'].batch(batch_size)\n",
        "ds['val'] = ds['val'].batch(batch_size)\n",
        "\n",
        "# Create a copy of the flood mask for each sample in the batch\n",
        "the_mask = np.array([the_mask]*batch_size)\n",
        "\n",
        "# Print the shape of the batched mask\n",
        "print (\"Shape of the mask: %s\" % str(the_mask.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da54f2b-1030-496e-91c8-eefbb8155119",
      "metadata": {
        "id": "4da54f2b-1030-496e-91c8-eefbb8155119"
      },
      "outputs": [],
      "source": [
        "name = MODEL_NAME+\"_split_{}_{}\".format(str(split), time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "#TF Callbacks\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='models/logs/{}'.format(name), histogram_freq=1)\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"./models/trained_models/%s/initial/\" % MODEL_NAME,\n",
        "                    monitor=\"val_loss\", mode=\"min\", save_weights_only=True,\n",
        "                    save_best_only=True, verbose=1)\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = EPOCHS//5)\n",
        "\n",
        "steps_per_epoch = int(ds[\"train\"].cardinality())#//batch_size\n",
        "lr_schedule = LinearDecayPerEpoch(LR, steps_per_epoch, EPOCHS, MIN_LR)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.85, patience=10, min_lr=MIN_LR)\n",
        "first_decay_steps = steps_per_epoch*(EPOCHS//3)\n",
        "cosine_lr_restarts = tf.keras.optimizers.schedules.CosineDecayRestarts(LR,first_decay_steps, t_mul=1.0, m_mul=0.8)\n",
        "\n",
        "# Compute the number of warmup batches\n",
        "warmup_batches = WARMUP_EPOCHS * steps_per_epoch\n",
        "# Create the Learning rate scheduler\n",
        "warm_up_lr = WarmUpLearningRateScheduler(warmup_batches, init_lr=LR)\n",
        "\n",
        "# Model Fitting\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "#opt = tfa.optimizers.AdamW(learning_rate=init_lr, weight_decay=0.0005)\n",
        "\n",
        "if \"CASP\" in MODEL_NAME:\n",
        "    if \"_\" not in MODEL_NAME:\n",
        "        model = new_models.CASPIAN(input_shape=(grid_size, grid_size, 1), filters=72, input_centering=True,\n",
        "                        depth=4, cardinality=34, activation='tanh', bottleneck_depth=8,\n",
        "                        init=\"glorot_normal\", sup_level=1, compression_factor= 0.85, gr_level= 4)\n",
        "    else:\n",
        "        model = new_models.CASPIAN_beta(input_shape=(grid_size, grid_size, 1), filters=72, input_centering=True,\n",
        "                depth=4, cardinality=34, activation='tanh', bottleneck_depth=8,\n",
        "                init=\"glorot_normal\", sup_level=1, compression_factor= 0.85, gr_level= 4, version=version)\n",
        "elif \"SWIN\" in MODEL_NAME:\n",
        "    set_seed()\n",
        "    model = new_models.Swin_unet(\n",
        "        filter_num_begin = 64,\n",
        "        depth = 4,\n",
        "        stack_num_down = 2,\n",
        "        stack_num_up = 2,\n",
        "        patch_size = 8,\n",
        "        att_heads = 4,\n",
        "        input_centering=True,\n",
        "        dropout = 0.0)\n",
        "else:\n",
        "    if \"pretrained\" in MODEL_NAME:\n",
        "        model = models.att_unet_2d((1024, 1024, 3), filter_num=[32, 64, 128, 256], n_labels=1,\n",
        "                               stack_num_down=2, stack_num_up=2, activation='ReLU',\n",
        "                               atten_activation='ReLU', attention='add', output_activation='ReLU',\n",
        "                               batch_norm=False, pool=True, unpool=False,\n",
        "                               backbone='VGG19', weights='imagenet',\n",
        "                               freeze_backbone=False, freeze_batch_norm=False,\n",
        "                               name='attunet')\n",
        "    else:\n",
        "        model = models.att_unet_2d((1024, 1024, 3), filter_num=[32, 64, 128, 256], n_labels=1,\n",
        "                               stack_num_down=2, stack_num_up=2, activation='ReLU',\n",
        "                               atten_activation='ReLU', attention='add', output_activation='ReLU',\n",
        "                               batch_norm=False, pool=True, unpool=False,\n",
        "                               backbone='VGG19', weights=None,\n",
        "                               freeze_backbone=False, freeze_batch_norm=False,\n",
        "                               name='attunet')\n",
        "\n",
        "\n",
        "if output_1d:\n",
        "    model.compile(loss=tf.keras.losses.Huber(delta=0.5), optimizer=opt, metrics=[tf.keras.metrics.RootMeanSquaredError(), \"mae\"])\n",
        "else:\n",
        "    model.compile(loss=custom_loss(mask=the_mask), optimizer=opt, metrics=[custom_rmse(mask=the_mask), custom_mae(mask=the_mask)])\n",
        "\n",
        "# Print a summary of the model architecture (layers, parameters, etc.)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# First Training Phase: Warmup\n",
        "# -----------------------------------------\n",
        "\n",
        "# Train the model for the full number of epochs (full training)\n",
        "# Continue training from the warmup weights\n",
        "# Callbacks here:\n",
        "# - checkpoint: keep saving good models\n",
        "# - tensorboard_callback: keep logging metrics\n",
        "# - early_stop: stop training early if validation loss stops improving\n",
        "# - reduce_lr: lower the learning rate when plateau is detected\n",
        "\n",
        "history_warmup = model.fit(ds['train'],\n",
        "                epochs=WARMUP_EPOCHS,\n",
        "                validation_data=ds['val'],\n",
        "                callbacks=[checkpoint, tensorboard_callback, warm_up_lr]) #PrintLearningRate()#reduce_lr#early_stop\n",
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# After Warmup Phase: Load Best Warmup Weights\n",
        "# -----------------------------------------\n",
        "\n",
        "\n",
        "# Reload the model weights saved after the warmup phase.\n",
        "# This ensures training continues from the best model checkpoint after warmup (instead of random weights).\n",
        "model.load_weights(\"./models/trained_models/%s/initial/\" % MODEL_NAME)\n",
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# Second Training Phase: Full Training\n",
        "# -----------------------------------------\n",
        "\n",
        "\n",
        "# Continue training the model starting from the loaded warmup weights.\n",
        "# Now train for the full number of epochs defined by EPOCHS.\n",
        "# Use callbacks:\n",
        "# - checkpoint: continue saving best models\n",
        "# - tensorboard_callback: continue logging metrics\n",
        "# - early_stop: stop training early if validation loss stops improving (avoids overfitting)\n",
        "# - reduce_lr: reduce the learning rate if validation loss plateaus\n",
        "history = model.fit(ds['train'],\n",
        "                epochs=EPOCHS,\n",
        "                validation_data=ds['val'],\n",
        "                callbacks=[checkpoint, tensorboard_callback, early_stop, reduce_lr]) #PrintLearningRate()#reduce_lr#early_stop\n",
        "\n",
        "# -----------------------------------------\n",
        "# Finalizing: Load Best Full Training Weights\n",
        "# -----------------------------------------\n",
        "\n",
        "# Load again the best model weights obtained during the full training phase.\n",
        "# This ensures the model saved in the next step is the best-performing version (lowest validation loss).\n",
        "model.load_weights(\"./models/trained_models/%s/initial/\" % MODEL_NAME)\n",
        "\n",
        "\n",
        "# -----------------------------------------\n",
        "# Saving the Fully Trained Model\n",
        "# -----------------------------------------\n",
        "\n",
        "\n",
        "# Save the fully trained model to disk in HDF5 (.h5) format.\n",
        "# The filename includes the MODEL_NAME and the data split used (e.g., split 1, 2, or 3).\n",
        "model.save(\"./models/trained_models/\"+MODEL_NAME+\"_split_{}\".format(str(split)), save_format='h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (base)",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
